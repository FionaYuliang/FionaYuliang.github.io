<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>数组内置排序函数对比</title>
    <link href="/2021/03/16/%E6%95%B0%E7%BB%84%E5%86%85%E7%BD%AE%E6%8E%92%E5%BA%8F%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/"/>
    <url>/2021/03/16/%E6%95%B0%E7%BB%84%E5%86%85%E7%BD%AE%E6%8E%92%E5%BA%8F%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</url>
    
    <content type="html"><![CDATA[<p>PHP 手册专门列出了数组内置排序函数的属性一览表, 不过仔细看就会发现, 这个表格比较乱, 缺乏条理性</p><p><img src="http://tva1.sinaimg.cn/large/006hVAtMly1h7sc3ccty4j310s0hg10u.jpg" alt="图片"></p><p>因此我对这些排序函数做了简单的归类和对比, 这样看起来一目了然, 也仍容易记忆使用了.</p><p><img src="https://uploader.shimo.im/f/8vzoLOQmKVQY6nd0.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="图片"></p><p>首先带有 sort 的分为 3 组 6 对, 每对之间是正序&#x2F;倒序的相反关系,其他属性相同.</p><p>sort 和 asort 的区别是:是否在排序时保持键值关联</p><p>只有 sort&#x2F;rsort&#x2F;shuffle 排序不需要考虑键的问题,可随意使用</p><p>用户可自定义排序函数 :usort&#x2F;uasort&#x2F;uksort</p><p>natsort&#x2F;natcasesort 后者的 case 指的是忽略大小写</p><p>array_multisort 的基本排序方式是: arrayA 按照键值大小排序,然后其他 array 都按照 arrayA 的调整策略跟着调整</p><p>参考:<a href="https://www.php.net/manual/zh/array.sorting.php">https://www.php.net/manual/zh/array.sorting.php</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>PHP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>github-hexo 两小时快速搭建个人博客网站</title>
    <link href="/2021/03/15/github-hexo%20%E4%B8%A4%E5%B0%8F%E6%97%B6%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"/>
    <url>/2021/03/15/github-hexo%20%E4%B8%A4%E5%B0%8F%E6%97%B6%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<p>今天用 2 个小时的时间完成了 Hexo 个人博客的最基本搭建，因此博客的第一篇文章就是与此有关。</p><p>此博客的搭建使用的是 CSDN 上看到的一篇教程：</p><p><a href="https://blog.csdn.net/sinat_37781304/article/details/82729029">https://blog.csdn.net/sinat_37781304&#x2F;article&#x2F;details&#x2F;82729029</a></p><p>该教程名副其实，或许可以称得上“史上最全”，分为三个部分：</p><p>第一部分：hexo 的初级搭建还有部署到 github page 上，以及个人域名的绑定。</p><p>第二部分：hexo 的基本配置，更换主题，实现多终端工作，以及在 coding page 部署实现国内外分流</p><p>第三部分：hexo 添加各种功能，包括搜索的 SEO，阅读量统计，访问量统计和评论系统等。</p><p>此博客部署完成后，如你所见，使用的 github 给定域名，暂未设置个人域名。然后对网站的语言、标题、风格主题等进行了设置。其他扩展功能暂未上线，可以满足最基本的需要。</p><p>以下是 Hexo 搭建步骤，点击教程做起来吧，你也可以轻松拥有自己的网站。</p><ol><li>安装 Git</li><li>安装 Node.js</li><li>安装 Hexo</li><li>GitHub 创建个人仓库</li><li>生成 SSH 添加到 GitHub</li><li>将 hexo 部署到 GitHub</li><li>设置个人域名</li><li>发布文章</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>github, Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python爬虫之：抓取B站高分番剧、视频弹幕图云</title>
    <link href="/2020/08/02/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%8A%93%E5%8F%96B%E7%AB%99%E9%AB%98%E5%88%86%E7%95%AA%E5%89%A7%E3%80%81%E8%A7%86%E9%A2%91%E5%BC%B9%E5%B9%95%E5%9B%BE%E4%BA%91/"/>
    <url>/2020/08/02/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%8A%93%E5%8F%96B%E7%AB%99%E9%AB%98%E5%88%86%E7%95%AA%E5%89%A7%E3%80%81%E8%A7%86%E9%A2%91%E5%BC%B9%E5%B9%95%E5%9B%BE%E4%BA%91/</url>
    
    <content type="html"><![CDATA[<h1 id="1-抓取高分番剧"><a href="#1-抓取高分番剧" class="headerlink" title="1.抓取高分番剧"></a>1.抓取高分番剧</h1><h2 id="原始网页"><a href="#原始网页" class="headerlink" title="原始网页"></a>原始网页</h2><p><img src="https://uploader.shimo.im/f/GbrF2IqozjV1j7v0.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="番剧列表页"></p><p><a href="https://www.bilibili.com/anime/index/#season_version=-1&area=-1&is_finish=-1&copyright=-1&season_status=-1&season_month=-1&year=-1&style_id=-1&order=4&st=1&sort=0&page=1">https://www.bilibili.com/anime/index/#season_version&#x3D;-1&amp;area&#x3D;-1&amp;is_finish&#x3D;-1&amp;copyright&#x3D;-1&amp;season_status&#x3D;-1&amp;season_month&#x3D;-1&amp;year&#x3D;-1&amp;style_id&#x3D;-1&amp;order&#x3D;4&amp;st&#x3D;1&amp;sort&#x3D;0&amp;page&#x3D;1</a></p><h2 id="python-爬虫代码（储存到数据库）"><a href="#python-爬虫代码（储存到数据库）" class="headerlink" title="python 爬虫代码（储存到数据库）"></a>python 爬虫代码（储存到数据库）</h2><h3 id="主文件-lt-niche-gems-py-gt"><a href="#主文件-lt-niche-gems-py-gt" class="headerlink" title="主文件 &lt;niche_gems.py&gt;"></a>主文件 &lt;niche_gems.py&gt;</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs plain">import requests<br>from bs4 import BeautifulSoup<br>import json<br>import time<br>def getReviewTimes(media_link):<br>    res = requests.get(media_link)<br>    soup = BeautifulSoup(res.text, &#x27;html.parser&#x27;)<br>    review_times = soup.find_all(&#x27;div&#x27;, class_=&#x27;media-info-review-times&#x27;)<br>    for i in review_times:<br>        review_time = i.text<br>    return review_time<br>def getAnime(url):<br>    res = requests.get(url)<br>    graded_data = json.loads(res.text)<br>    animes_list = graded_data[&#x27;data&#x27;][&#x27;list&#x27;]<br>    media_ids = []<br>    media_links = []<br>    m_orders = []<br>    titles = []<br>    review_times = []<br>    for each in animes_list:<br>        media_id = each[&#x27;media_id&#x27;]<br>        media_ids.append(media_id)<br>        media_link = f&#x27;https://www.bilibili.com/bangumi/media/md&#123;media_id&#125;&#x27;<br>        review_time = getReviewTimes(media_link)<br>        review_times.append(review_time)<br>        media_links.append(media_link)<br>        m_order = each[&#x27;order&#x27;].strip()<br>        m_orders.append(m_order)<br>        titles.append(each[&#x27;title&#x27;])<br>    animes_matrix = [media_ids, m_orders, titles, review_times, media_links]<br>    animes_result = list(zip(*animes_matrix))<br>    return animes_result<br></code></pre></td></tr></table></figure><h3 id="按照评分-by-score"><a href="#按照评分-by-score" class="headerlink" title="按照评分 by score"></a>按照评分 by score</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs plain"><br>import niche_gems<br>import sqlite3<br>import csv<br>import time<br>conn = sqlite3.connect(&#x27;anime.db&#x27;)<br>c = conn.cursor()<br>def createTable():<br>    c.execute(&quot;&quot;&quot;CREATE TABLE IF NOT EXISTS animeByScore(<br>        media_id INTEGER PRIMARY KEY,<br>        m_order REAL,<br>        title TEXT,<br>        review_time INTEGER,<br>        media_link TEXT)&quot;&quot;&quot;)<br>def main():<br>    createTable()<br>    for i in range(1, 90):<br>        print(f&#x27;page &#123;i&#125; scraping...&#x27;)<br>        url_score = f&#x27;https://api.bilibili.com/pgc/season/index/result?season_version=-1&amp;area=-1&amp;is_finish=-1&amp;copyright=-1&amp;season_status=-1&amp;season_month=-1&amp;year=-1&amp;style_id=-1&amp;order=4&amp;st=1&amp;sort=0&amp;page=&#123;i&#125;&amp;season_type=1&amp;pagesize=20&amp;type=1&#x27;<br>        time.sleep(1)<br>        animes_results_by_score = []<br>        animes_result_by_score = niche_gems.getAnime(url_score)<br>        animes_results_by_score.extend(animes_result_by_score)<br>        c.executemany(&#x27;INSERT OR IGNORE INTO animeByScore VALUES(?,?,?,?,?)&#x27;,<br>                      animes_results_by_score)<br>        conn.commit()<br>        print(f&#x27;page &#123;i&#125; animeByScore insert successfully...&#x27;)<br>        print(f&#x27;page &#123;i&#125; done...&#x27;)<br>    c.close()<br>    conn.close()<br>    print(&#x27;执行完毕，数据库已关闭！&#x27;)<br>if __name__ == &quot;__main__&quot;:<br>    main()<br></code></pre></td></tr></table></figure><h3 id="按照播放量-by-play-count"><a href="#按照播放量-by-play-count" class="headerlink" title="按照播放量 by play count"></a>按照播放量 by play count</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs plain">import niche_gems<br>import sqlite3<br>conn = sqlite3.connect(&#x27;anime.db&#x27;)<br>c = conn.cursor()<br>def createTable():<br>    c.execute(&quot;&quot;&quot;CREATE TABLE IF NOT EXISTS animeByPlaycount(<br>        media_id int primary key，<br>        order int,<br>        title text,<br>        review_time int,<br>        media_link text<br>    )&quot;&quot;&quot;)<br>def query_and_output():<br>    c.execute(&#x27;SELECT * from animeByPlaycount SORT BY order&#x27;)<br>    data = c.fetchall()<br>    c.close()<br>    conn.close()<br>    with open(&#x27;/Users/yao/www/python//bilibili/anime/animeByPlaycount.txt&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as file:<br>        for row in data:<br>            file.write()<br>def main():<br>    url_play_count = &#x27;https://api.bilibili.com/pgc/season/index/result?season_version=-1&amp;area=-1&amp;is_finish=-1&amp;copyright=-1&amp;season_status=-1&amp;season_month=-1&amp;year=-1&amp;style_id=-1&amp;order=2&amp;st=1&amp;sort=0&amp;page=1&amp;season_type=1&amp;pagesize=20&amp;type=1&#x27;<br>    animes_result_by_playcount = niche_gems.getAnime(url_play_count)<br>    createTable()<br>    c.executemany(&#x27;INSERT OR IGNORE INTO animeByPlaycount&#x27;)<br>    conn.commit()<br>    print(&#x27;animeByPlaycount insert successfully...&#x27;)<br>    query_and_output()<br>    print(&#x27;animeByPlaycount file saved successfully...&#x27;)<br>if __name__ == &quot;__main__&quot;:<br>    main()<br></code></pre></td></tr></table></figure><h2 id="数据库内容"><a href="#数据库内容" class="headerlink" title="数据库内容"></a>数据库内容</h2><p><img src="https://uploader.shimo.im/f/W9vxXsoi1Nw4XG2b.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="animate.db/animateByScore"></p><h2 id="从数据库中取出并展示在-html-中"><a href="#从数据库中取出并展示在-html-中" class="headerlink" title="从数据库中取出并展示在 html 中"></a>从数据库中取出并展示在 html 中</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs plain">import sqlite3<br>import csv<br>import pandas as pd<br>from prettytable import PrettyTable<br>conn = sqlite3.connect(&#x27;anime.db&#x27;)<br>c = conn.cursor()<br>c.execute(&#x27;SELECT * from animeByScore WHERE m_order &gt; 9.7&#x27;)<br>data = c.fetchall()<br>c.close()<br>conn.close()<br># file_path = &#x27;/Users/yao/www/python/bilibili/anime/animeByScore.csv&#x27;<br># with open(file_path, &#x27;w&#x27;, newline=&#x27;&#x27;, encoding=&#x27;utf-8&#x27;) as f:<br>#     fieldnames = [&#x27;anime_id&#x27;, &#x27;m_order&#x27;, &#x27;title&#x27;, &#x27;review_time&#x27;, &#x27;anime_link&#x27;]<br>#     f_csv = csv.DictWriter(f, fieldnames=fieldnames)<br>#     f_csv.writeheader()<br>#     for row in data:<br>#         f_csv.writerow(<br>#             &#123;<br>#                 &#x27;anime_id&#x27;: row[0],<br>#                 &#x27;m_order&#x27;: row[1],<br>#                 &#x27;title&#x27;: row[2],<br>#                 &#x27;review_time&#x27;: row[3],<br>#                 &#x27;anime_link&#x27;: row[4]<br>#             &#125;<br>#         )<br>html_uri = &#x27;/Users/yao/www/python/bilibili/anime/animeByScore.html&#x27;<br>record_list = data<br>title = &quot;评分列表&quot;<br>tbody_content = &quot;&quot;<br>for record in record_list:<br>    tbody_content = tbody_content + f&quot;&quot;&quot;<br>     &lt;tr&gt;<br>        &lt;td&gt;&#123;record[0]&#125;&lt;/td&gt;<br>        &lt;td&gt;&#123;record[1]&#125;&lt;/td&gt;<br>        &lt;td&gt;&#123;record[2]&#125;&lt;/td&gt;<br>        &lt;td&gt;&#123;record[3]&#125;&lt;/td&gt;<br>        &lt;td&gt;&#123;record[4]&#125;&lt;/td&gt;<br>    &lt;/tr&gt;<br>    &quot;&quot;&quot;<br>content = f&quot;&quot;&quot;<br>&lt;table&gt;<br>&lt;thead&gt;<br>    &lt;th&gt;anime_id&lt;/th&gt;<br>    &lt;th&gt;m_order&lt;/th&gt;<br>    &lt;th&gt;title&lt;/th&gt;<br>    &lt;th&gt;review_time&lt;/th&gt;<br>    &lt;th&gt;anime_link&lt;/th&gt;<br>&lt;/thead&gt;<br>&lt;tbody&gt;<br>    &#123;tbody_content&#125;<br>&lt;/tbody&gt;<br>&lt;/table&gt;<br>&quot;&quot;&quot;<br>html = f&quot;&quot;&quot;<br>&lt;!DOCTYPE html&gt;<br>&lt;html lang=&quot;zh-CN&quot;&gt;<br>&lt;head&gt;<br>&lt;meta charset=&quot;utf-8&quot;&gt;<br>&lt;title&gt;&#123;title&#125;&lt;/title&gt;<br>&lt;/head&gt;<br>&lt;body&gt;<br>&#123;content&#125;<br>&lt;/body&gt;<br>&lt;/html&gt;<br>&quot;&quot;&quot;<br>with open(html_uri, &#x27;w&#x27;, newline=&#x27;&#x27;, encoding=&#x27;utf-8&#x27;) as f:<br>    f.write(html)<br>print(&#x27;csv file saved (＾－＾)V&#x27;)<br></code></pre></td></tr></table></figure><h2 id="html-展示效果"><a href="#html-展示效果" class="headerlink" title="html 展示效果"></a>html 展示效果</h2><p><img src="https://uploader.shimo.im/f/7piCCurNzIDREWHa.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="生成的html"></p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="小技巧：如何把-CSV-转换成-HTML"><a href="#小技巧：如何把-CSV-转换成-HTML" class="headerlink" title="小技巧：如何把 CSV 转换成 HTML"></a>小技巧：如何把 CSV 转换成 HTML</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs plain">from prettytable import PrettyTable<br>file_path = &#x27;/Users/yao/www/python/bilibili/anime/animeByScore.csv&#x27;<br>csv_file = open(file_path, &#x27;r&#x27;)<br>csv_file = csv_file.readlines()<br>tb = PrettyTable(csv_file[0].split(&#x27;,&#x27;))<br># 这一句也可以这么写<br># table = PrettyTable()<br># table.field_names=csv_file[0].split(&#x27;,&#x27;)<br>for row in range(1, 100):<br>    csv_file[row] = csv_file[row].split(&#x27;,&#x27;)<br>    tb.add_row(csv_file[row])<br>html_path = &#x27;/Users/yao/www/python/bilibili/anime/html_file.html&#x27;<br>html_file = open(html_path, &#x27;w&#x27;)<br>html_code = tb.get_html_string()<br>html_file = html_file.write(html_code)<br></code></pre></td></tr></table></figure><h1 id="2-抓取视频弹幕并制作图云"><a href="#2-抓取视频弹幕并制作图云" class="headerlink" title="2.抓取视频弹幕并制作图云"></a>2.抓取视频弹幕并制作图云</h1><h2 id="python-爬虫弹幕"><a href="#python-爬虫弹幕" class="headerlink" title="python 爬虫弹幕"></a>python 爬虫弹幕</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs plain">import requests<br>import json<br>import re<br><br>def get_cid(url):<br>    bvid = url.split(&#x27;/&#x27;)[4].split(&#x27;?&#x27;)[0]<br>    danmu_page_link = f&#x27;https://api.bilibili.com/x/player/pagelist?bvid=&#123;bvid&#125;&amp;jsonp=jsonp&#x27;<br>    res = requests.get(danmu_page_link)<br>    cid_page = json.loads(res.text)<br>    cid = cid_page[&#x27;data&#x27;][0][&#x27;cid&#x27;]<br>    # 获取cid时要注意视频是不是多p,自己做小工具时用其中1p就可以<br>    # cids = res_dict[&#x27;data&#x27;][&#x27;cid&#x27;]<br>    # part_names = res_dict[&#x27;data&#x27;][&#x27;part&#x27;]<br>    return cid<br>def get_danmu(cid):<br>    danmu_url = f&#x27;https://api.bilibili.com/x/v1/dm/list.so?oid=&#123;cid&#125;&#x27;<br>    res = requests.get(danmu_url)<br>    res_xml = res.content.decode(&#x27;utf-8&#x27;)<br>    pattern = re.compile(&#x27;&lt;d.*?&gt;(.*?)&lt;/d&gt;&#x27;)<br>    danmu_list = pattern.findall(res_xml)<br>    return danmu_list<br>def save_file(danmu_list):<br>    file_path = &#x27;/Users/yao/www/python/bilibili/video/danmu_file.txt&#x27;<br>    with open(file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as file:<br>        for item in danmu_list:<br>            file.write(item)<br>            file.write(&#x27;\n&#x27;)<br>def main():<br>    source = &#x27;https://www.bilibili.com/video/BV1xs411Q799?p=1&#x27;<br>    cid = get_cid(source)<br>    danmu_list = get_danmu(cid)<br>    save_file(danmu_list)<br>    print(&#x27;file saved successfully...&#x27;)<br>if __name__ == &#x27;__main__&#x27;:<br>    main()<br></code></pre></td></tr></table></figure><h3 id="-1"><a href="#-1" class="headerlink" title=""></a></h3><h2 id="得到的弹幕文本"><a href="#得到的弹幕文本" class="headerlink" title="得到的弹幕文本"></a>得到的弹幕文本</h2><p><img src="https://uploader.shimo.im/f/bmL7w8bGEIt6pI8V.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="弹幕文本"></p><h3 id="弹幕文本图云分析"><a href="#弹幕文本图云分析" class="headerlink" title="弹幕文本图云分析"></a>弹幕文本图云分析</h3><h3 id="分词代码"><a href="#分词代码" class="headerlink" title="分词代码"></a>分词代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plain">import jieba<br>from wordcloud import WordCloud<br>file_path = &#x27;/Users/yao/www/python/bilibili/video/danmu_file.txt&#x27;<br>with open(file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as file:<br>    text_str = file.read()<br>seg_list = jieba.cut(text_str)<br>word_str = &quot; &quot;.join(seg_list)<br>font_path = &quot;/System/Library/Fonts/PingFang.ttc&quot;<br>wc_settintg = &#123;<br>    &#x27;font_path&#x27;: &#x27;/System/Library/Fonts/PingFang.ttc&#x27;,<br>    &#x27;background_color&#x27;: &#x27;white&#x27;,<br>    &#x27;width&#x27;: 1000,<br>    &#x27;height&#x27;: 860,<br>    &#x27;margin&#x27;: 2,<br>&#125;<br>wc = WordCloud(**wc_settintg).generate(word_str)<br>wc.to_file(&#x27;/Users/yao/www/python/bilibili/video/xiaojiayu_python_p1.png&#x27;)<br>print(&#x27;wordcloud done&#x27;)<br></code></pre></td></tr></table></figure><p>参考视频：<a href="https://www.bilibili.com/video/BV1g7411e7m4">https://www.bilibili.com/video/BV1g7411e7m4</a></p><h3 id="得到的图云"><a href="#得到的图云" class="headerlink" title="得到的图云"></a>得到的图云</h3><p>（B 站的好多弹幕真是越来越不能看了……）</p><p><img src="https://uploader.shimo.im/f/DvLCgWmCYqB4RbJl.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="图片"></p><h3 id="遇到的问题：生成的图片-中文乱码"><a href="#遇到的问题：生成的图片-中文乱码" class="headerlink" title="遇到的问题：生成的图片 中文乱码"></a>遇到的问题：生成的图片 中文乱码</h3><p>查了一下这是因为中文字体包不适配，然后发现我们用 mac 自带的苹方字体就能非常简单的搞定。做词云的时候用的 Mac，所以 Windows 的解决方法没有尝试。</p><p>本地路径是：</p><p>font_path &#x3D; “&#x2F;System&#x2F;Library&#x2F;Fonts&#x2F;PingFang.ttc”</p><p><img src="https://uploader.shimo.im/f/gnsjnKTW75l7CnJb.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="本地路径"></p><p>在电脑中查看：</p><ol><li>启动台——其他——打开字体册</li></ol><p><img src="https://uploader.shimo.im/f/IXg7PFlvLltkqEYG.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="Mac的字体册"></p><ol start="2"><li>如果你之前没有改过系统字体的话，默认选中苹方简体，右键选择在 Finder 中显示</li></ol><p><img src="https://uploader.shimo.im/f/JNHIQbkInbc4WQWN.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="选择字体"></p><p><img src="https://uploader.shimo.im/f/ReT6XWuT8VPw0dTs.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="需要的字体包"></p><p>这就是我们需要的字体包。</p><p>如果不需要其他参数的话，字体路径直接传入 WordCloud()参数就可以了。</p><p>wc &#x3D; WordCloud(font_path).generate(word_str)</p><p>词云官方文档地址：<a href="https://github.com/amueller/word_cloud">https://github.com/amueller/word_cloud</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python爬虫之：某用户所有微博的简单备份</title>
    <link href="/2020/07/20/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%9F%90%E7%94%A8%E6%88%B7%E6%89%80%E6%9C%89%E5%BE%AE%E5%8D%9A%E7%9A%84%E7%AE%80%E5%8D%95%E5%A4%87%E4%BB%BD/"/>
    <url>/2020/07/20/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%9F%90%E7%94%A8%E6%88%B7%E6%89%80%E6%9C%89%E5%BE%AE%E5%8D%9A%E7%9A%84%E7%AE%80%E5%8D%95%E5%A4%87%E4%BB%BD/</url>
    
    <content type="html"><![CDATA[<h3 id="python-爬虫代码"><a href="#python-爬虫代码" class="headerlink" title="python 爬虫代码"></a>python 爬虫代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs plain"><br># 目的：编写简易微博备份工具, 将指定微博备份为txt文本<br># 1.  学习python使用<br># 2.  学习使用python获取web页面(学习http调用/了解简单网络请求方式/使用外部库)<br># 3.  使用python抓取微博接口(了解浏览器抓包方法, http基础规范)<br># 4.  将抓取接口存入数据库中(掌握本地数据库构建存取/sql的增删改查写法)<br># 5.  从数据库中读取接口, 输出txt文本<br>import requests<br>import json<br>import time<br>import sqlite3<br>from bs4 import BeautifulSoup<br>def get_page(url):<br>    headers = &#123;<br>        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36&#x27;,<br>        &#x27;referer&#x27;: &#x27;https://m.weibo.cn/u/6587600437&#x27;<br>    &#125;<br>    # target_url = &#x27;https://m.weibo.cn/u/1768825052?uid=1768825052&amp;luicode=10000011&amp;lfid=1076031768825052&#x27;<br>    res = requests.get(url, headers=headers)<br>    return res<br>def get_since_id(res):<br>    weibo_json = json.loads(res.text)<br>    since_id = int(weibo_json[&#x27;data&#x27;][&#x27;cardlistInfo&#x27;][&#x27;since_id&#x27;])<br>    return since_id<br>def get_weibo(res):<br>    weibo_json = json.loads(res.text)<br>    weibo_cards = weibo_json[&quot;data&quot;][&quot;cards&quot;]<br>    weibo_id = []<br>    weibo_time = []<br>    weibo_text = []<br>    reposts_count = []<br>    comments_count = []<br>    attitudes_count = []<br>    for each in weibo_cards:<br>        weibo_id.append(each[&quot;mblog&quot;][&quot;id&quot;])<br>        weibo_text.append(each[&quot;mblog&quot;][&quot;text&quot;])<br>        weibo_time.append(each[&#x27;mblog&#x27;][&#x27;created_at&#x27;])<br>        reposts_count.append(each[&#x27;mblog&#x27;][&#x27;reposts_count&#x27;])<br>        comments_count.append(each[&#x27;mblog&#x27;][&#x27;comments_count&#x27;])<br>        attitudes_count.append(each[&#x27;mblog&#x27;][&#x27;attitudes_count&#x27;])<br>    weibo_matrix = [weibo_id, weibo_time, weibo_text,<br>                    reposts_count, comments_count, attitudes_count]<br>    result = list(zip(*weibo_matrix))<br>    return result<br>conn = sqlite3.connect(&#x27;weibo.db&#x27;)<br>c = conn.cursor()<br>def create_table():<br>    c.execute(&quot;&quot;&quot;CREATE TABLE IF NOT EXISTS weibo_person(<br>        weibo_id int,<br>        created_at text,<br>        weibo_text text,<br>        reposts_count int,<br>        comments_count int,<br>        attitudes_count int<br>        )&quot;&quot;&quot;)<br>def query_and_output():<br>    c.execute(&#x27;SELECT * FROM weibo_person&#x27;)<br>    data = c.fetchall()<br>    c.close()<br>    conn.close()<br>    with open(&#x27;/Users/yao/www/python/微博/leftchenn.txt&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as file:<br>        for item in data:<br>            string = f&#x27;发布时间：&#123;item[1]&#125; \n 微博内容：&#123;item[2]&#125; \n 转发数：&#123;item[3]&#125; 评论数：&#123;item[4]&#125; 转发数：&#123;item[5]&#125; \n\n\n &#x27;<br>            file.write(string)<br>def main():<br>    host = &quot;https://m.weibo.cn/u/1768825052&quot;<br>    target_url = &#x27;https://m.weibo.cn/api/container/getIndex?uid=1768825052&amp;luicode=10000011&amp;lfid=1076031768825052&amp;type=uid&amp;value=1768825052&amp;containerid=1076031768825052&#x27;<br>    res = get_page(target_url)<br>    result = get_weibo(res)<br>    since_id = get_since_id(res)<br>    for i in range(10):<br>        print(&#x27;page &#123;&#125; scraping...&#x27;.format(i))<br>        new_url = f&#x27;https://m.weibo.cn/api/container/getIndex?uid=1768825052&amp;luicode=10000011&amp;lfid=1076031768825052&amp;type=uid&amp;value=1768825052&amp;containerid=1076031768825052&amp;since_id=&#123;since_id&#125;&#x27;<br>        time.sleep(1)<br>        res = get_page(new_url)<br>        result.extend(get_weibo(res))<br>        since_id = get_since_id(res)<br>        print(&#x27;page &#123;&#125; scrap done...&#x27;.format(i))<br>    create_table()<br>    c.executemany(&#x27;INSERT INTO weibo_person VALUES(?,?,?,?,?,?)&#x27;, result)<br>    conn.commit()<br>    print(&#x27;insert successfully...&#x27;)<br>    query_and_output()<br>    print(&#x27;output successfully...&#x27;)<br>if __name__ == &quot;__main__&quot;:<br>    main()<br></code></pre></td></tr></table></figure><h3 id="数据库内容"><a href="#数据库内容" class="headerlink" title="数据库内容"></a>数据库内容</h3><p><img src="https://uploader.shimo.im/f/C4lQEFWbuu75xLc6.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="weibo.db/weibo_person"></p><h3 id="得到的文本"><a href="#得到的文本" class="headerlink" title="得到的文本"></a>得到的文本</h3><p><img src="https://uploader.shimo.im/f/1gkiCal2TQ1uG2ue.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="微博文本"></p>]]></content>
    
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python爬虫之：抓取网易云音乐某首歌曲的热门评论</title>
    <link href="/2020/07/16/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%8A%93%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E6%9F%90%E9%A6%96%E6%AD%8C%E6%9B%B2%E7%9A%84%E7%83%AD%E9%97%A8%E8%AF%84%E8%AE%BA/"/>
    <url>/2020/07/16/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%8A%93%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E6%9F%90%E9%A6%96%E6%AD%8C%E6%9B%B2%E7%9A%84%E7%83%AD%E9%97%A8%E8%AF%84%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h3 id="首先，以这首歌为例："><a href="#首先，以这首歌为例：" class="headerlink" title="首先，以这首歌为例："></a>首先，以这首歌为例：</h3><p><img src="https://uploader.shimo.im/f/uRWUHpnCGtVrJUzX.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="图片"></p><h3 id="python-代码如下："><a href="#python-代码如下：" class="headerlink" title="python 代码如下："></a>python 代码如下：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs plain">import requests<br>import json<br># import BeautifulSoup from bs4<br>def get_hotComments(res):<br>    comments_json = json.loads(res.text)<br>    hot_comments = comments_json[&#x27;hotComments&#x27;]<br>    with open(&#x27;/Users/yao/www/python/evol-love.txt&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as file:<br>        for each in hot_comments:<br>            file.write(each[&#x27;user&#x27;][&#x27;nickname&#x27;] + &#x27;: \n\n&#x27;)<br>            file.write(each[&#x27;content&#x27;] + &#x27;\n&#x27;)<br>            # file.write(each[&#x27;likedCount&#x27;] + &#x27;\n&#x27;)<br>            # file.write(each[&#x27;time&#x27;] + &#x27;\n&#x27;)<br>            file.write(&#x27;--❤-----------❤--\n&#x27;)<br>def get_comments(url):<br>    song_id = url.split(&#x27;=&#x27;)[1]<br>    headers = &#123;<br>        &#x27;user-agent&#x27;: &#x27;curl https://bootstrap.pypa.io/get-pip.py | python3&#x27;,<br>        &#x27;referer&#x27;: &#x27;https://music.163.com/song?id=1381290746&#x27;&#125;<br>    params = &#x27;15S1vG2hDWPcsAeXpjYIcwuRpK7nwElk3Cdy6c/3aJdbH5aBub7nWrYPxZQaMzKrhFSUo1n4nGHkCbaZAl77WSFPMYjjv6sU5Qipnqnn4QjXRH6eN1w/PcauCCUo4NDgaw/Uudad5J/zpgDY2005gC3UF+s2s/ND5tjXTU0SeDkiBg47A6b6VqhK/7E/hS9++nlvAC7QkkzCbsXZMp/jyBl5+KroeMcGAfFumxZ5pME=&#x27;<br>    encSecKey = &#x27;3f5dbdd0ec1f0a1501b88f6e04ce30ecf88192b4e3a92bd2dc1216f2ccc90898211e6347a2ab36d07f93793352ff6559fb927b7cbd337f5887d2af7b442e2dfb0b4abd83e85c6c3bb20fe2beef050db4c169531411f4e23caaa201f62609fcda4de694c8e9e759443fea2f7febf044bd9c4bcb2697519ca96dca676730059156&#x27;<br>    data = &#123;<br>        &#x27;params&#x27;: params,<br>        &#x27;encSecKey&#x27;: encSecKey<br>    &#125;<br>    target_url = &#x27;https://music.163.com/weapi/v1/resource/comments/R_SO_4_&#123;&#125;?csrf_token=55a6b8514346a9f676c8485978093b9f&#x27;.format(song_id)<br>    res = requests.post(target_url, headers=headers, data=data)<br>    # rawHTML =  &#x27;&#x27;<br>    # soup = BeautifulSoup(rawHTML)<br>    # file_name_obj = soup.select_one(&#x27;.f-ff2&#x27;)<br>    # file_name = file_name_obj.text<br>    return res<br>def main():<br>    url = input(&#x27;please input song url:&#x27;)<br>    res = get_comments(url)<br>    # title = get_title(url)<br>    get_hotComments(res)<br>if __name__ == &quot;__main__&quot;:<br>    main()<br></code></pre></td></tr></table></figure><p>得到的txt文件如下（如有需要可进一步分析）：</p><p><img src="https://uploader.shimo.im/f/1DwdMQXCPNfJwB4s.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="图片"></p>]]></content>
    
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python爬虫之：抓取豆瓣电影TOP250</title>
    <link href="/2020/07/13/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%8A%93%E5%8F%96%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1TOP250/"/>
    <url>/2020/07/13/python%E7%88%AC%E8%99%AB%E4%B9%8B%EF%BC%9A%E6%8A%93%E5%8F%96%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1TOP250/</url>
    
    <content type="html"><![CDATA[<p>豆瓣用户每天都在对“看过”的电影进行“很差”到“力荐”的评价，豆瓣根据每部影片看过的人数以及该影片所得的评价等综合数据，通过算法分析产生豆瓣电影 Top 250。</p><h3 id="原始网页："><a href="#原始网页：" class="headerlink" title="原始网页："></a>原始网页：</h3><p><a href="https://movie.douban.com/top250">https://movie.douban.com/top250</a></p><p><img src="https://uploader.shimo.im/f/YwpUFMDrQ59YL1UA.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="图片"></p><h3 id="python-爬虫代码"><a href="#python-爬虫代码" class="headerlink" title="python 爬虫代码"></a>python 爬虫代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs plain">import requests<br>from bs4 import BeautifulSoup<br>import sqlite3<br>import time<br>def open_url(url):<br>    headers = &#123;<br>        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36&#x27;,<br>        &quot;Cookie&quot;: &#x27;bid=U5nCZ8p0dQU; gr_user_id=cb5f0143-dc7b-4558-8ca8-645601af3a4c; _vwo_uuid_v2=D8FFB9019ECC277DA812225E3B2109D06|9ff6373ff5de826c9b373e0499c2fd95; __gads=ID=fca53c413c37e6b3:T=1583725810:S=ALNI_Ma0O2oj2sLthssNt4FqeaZYp7EL_Q; ll=&quot;108288&quot;; douban-fav-remind=1; __yadk_uid=UtObTTkrwA7qukDAgdd2INMv9LH52PEA; viewed=&quot;26829016_26264642_26277694_4315606_1955110_1400498_4065258_1094802_26939853_12411215&quot;; __utmc=30149280; __utmc=223695111; dbcl2=&quot;33304907:UiHXJr1eiGI&quot;; ck=PYWO; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1591627160%2C%22https%3A%2F%2Faccounts.douban.com%2Fpassport%2Flogin%3Fredir%3Dhttps%253A%252F%252Fmovie.douban.com%252Ftop250%22%5D; _pk_ses.100001.4cf6=*; __utma=30149280.542734861.1583725808.1591614666.1591627161.51; __utmb=30149280.0.10.1591627161; __utmz=30149280.1591627161.51.46.utmcsr=accounts.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/passport/login; __utma=223695111.1397865339.1583730183.1591614666.1591627161.10; __utmb=223695111.0.10.1591627161; __utmz=223695111.1591627161.10.8.utmcsr=accounts.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/passport/login; push_noty_num=0; push_doumail_num=0; _pk_id.100001.4cf6=6441328ab4c5f495.1583730182.10.1591627294.1591614962.&#x27;,<br>        &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#x27;<br>    &#125;<br>    res = requests.get(url, headers=headers)<br>    return res<br>def find_moives(res):<br>    soup = BeautifulSoup(res.text, &#x27;html.parser&#x27;)<br>    # 电影名<br>    movies = []<br>    targets = soup.find_all(&#x27;div&#x27;, class_=&#x27;hd&#x27;)<br>    for each in targets:<br>        movies.append(each.a.span.text)<br>    # 评分<br>    ranks = []<br>    targets = soup.find_all(&#x27;em&#x27;)<br>    for each in targets:<br>        index = int(each.text)<br>        ranks.append(index)<br>    # 资料<br>    profiles = []<br>    targets = soup.find_all(&#x27;div&#x27;, class_=&#x27;bd&#x27;)<br>    for each in targets:<br>        try:<br>            profiles.append(each.p.text.split(&#x27;\n&#x27;)[<br>                1].strip() + each.p.text.split(&#x27;\n&#x27;)[2].strip())<br>        except:<br>            continue<br>    data_matrix = [ranks, movies, profiles]<br>    result = list(zip(*data_matrix))<br>    return result<br>def find_depth(res):<br>    soup = BeautifulSoup(res.text, &#x27;html.parser&#x27;)<br>    depth = soup.find(<br>        &#x27;span&#x27;, class_=&#x27;next&#x27;).previous_sibling.previous_sibling.text<br>    return int(depth)<br>conn = sqlite3.connect(&quot;douban_movies.db&quot;)<br>c = conn.cursor()<br>def create_table():<br>    c.execute(<br>        &quot;CREATE TABLE IF NOT EXISTS douban_movies250(item_id INT,rank INT, movie TEXT, profile TEXT)&quot;)<br>def query_and_output():<br>    c.execute(&#x27;SELECT * FROM douban_movies250&#x27;)<br>    data = c.fetchall()<br>    c.close()<br>    conn.close()<br>    with open(&#x27;/Users/yao/www/python/douban_movies250.txt&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as file:<br>        for row in data:<br>            string = f&#x27;Top &#123;row[1]&#125; : 《&#123;row[2]&#125;》，\n 简介： &#123;row[3]&#125;\n&#x27;<br>            file.write(string)<br>def main():<br>    host = &#x27;https://movie.douban.com/top250&#x27;<br>    res = open_url(host)<br>    depth = find_depth(res)<br>    result = []<br>    for i in range(depth):<br>        url = host + &#x27;/?start=&#x27; + str(25 * i)<br>        time.sleep(1)<br>        res = open_url(url)<br>        result.extend(find_moives(res))<br>    # item_id = [i for i in range(250)]<br>    # result_matrix = [item_id, result]<br>    # result = list(zip(*result_matrix))<br>    create_table()<br>    c.executemany(&#x27;INSERT INTO douban_movies250 VALUES(?,?,?,?)&#x27;, result)<br>    conn.commit()<br>    print(&#x27;insert database successfully&#x27;)<br>    query_and_output()<br>    print(&#x27;output successfully &#x27;)<br>if __name__ == &quot;__main__&quot;:<br>    main()<br></code></pre></td></tr></table></figure><h3 id="得到的文本"><a href="#得到的文本" class="headerlink" title="得到的文本"></a>得到的文本</h3><p><img src="https://uploader.shimo.im/f/ffvcAZnM2U1p7Y5e.png!thumbnail?fileGuid=xHYJXDKdjxTKJJHR" alt="图片"></p>]]></content>
    
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
